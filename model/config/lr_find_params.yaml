# config/lr_find_params.yaml
data:
  data_dir_train_val: "data/original"  # todo: Set the path to training/validation data
  num_classes: 4
  size: 224
  batch_size: 64 # todo: Keep same with the training batch size
  workers: 4
  k_fold_num_splits: 1
  validation_split_ratio: 0.2
  k_fold_random_seed: 12

model:
  model_name: "vit-b16-224-dino-v2"  # todo: Set model name
  # lr: 1e-7 # lr will be overridden by the finder's sweep
  optimizer: "adamw" # Needed for model.configure_optimizers()
  weight_decay: 0.01

trainer:
  accelerator: "auto"
  devices: "1"
  precision: "16-mixed"
  max_epochs: 30         # Max epochs for the LR find process itself

# todo: Set lr_find_params via command line